Absolutely! Let‚Äôs go **line by line** through your Terraform EKS setup (the production-ready version we just wrote, without a separate node security group). I‚Äôll explain every block in plain English so it‚Äôs easy to follow.

---

## **1Ô∏è‚É£ Provider**

```hcl
provider "aws" {
  region = "us-east-1"
}
```

* This tells Terraform **which cloud provider** to use (AWS).
* The region `us-east-1` means all resources will be created in **N. Virginia**.

---

## **2Ô∏è‚É£ VPC**

```hcl
resource "aws_vpc" "devopsshack_vpc" {
  cidr_block = "10.0.0.0/16"

  tags = {
    Name = "devopsshack-vpc"
  }
}
```

* Creates a **Virtual Private Cloud (VPC)** ‚Äî basically your isolated network in AWS.
* `cidr_block = "10.0.0.0/16"` ‚Üí network range; can hold up to 65,536 IP addresses.
* `tags` ‚Üí helps identify the resource in AWS Console.

---

## **3Ô∏è‚É£ Public Subnets**

```hcl
resource "aws_subnet" "devopsshack_public_subnet" {
  count                   = 2
  vpc_id                  = aws_vpc.devopsshack_vpc.id
  cidr_block              = cidrsubnet(aws_vpc.devopsshack_vpc.cidr_block, 8, count.index)
  availability_zone       = element(["us-east-1a", "us-east-1b"], count.index)
  map_public_ip_on_launch = true

  tags = {
    Name = "devopsshack-public-${count.index}"
  }
}
```

* Creates **2 public subnets** (one per availability zone).
* `count = 2` ‚Üí Terraform creates 2 subnets.
* `cidr_block = cidrsubnet(...)` ‚Üí splits VPC CIDR into smaller `/24` subnets automatically.
* `availability_zone` ‚Üí assigns subnet to `us-east-1a` and `us-east-1b`.
* `map_public_ip_on_launch = true` ‚Üí EC2 instances here automatically get public IPs.
* `tags` ‚Üí names the subnets.

---

## **4Ô∏è‚É£ Private Subnets**

```hcl
resource "aws_subnet" "devopsshack_private_subnet" {
  count             = 2
  vpc_id            = aws_vpc.devopsshack_vpc.id
  cidr_block        = cidrsubnet(aws_vpc.devopsshack_vpc.cidr_block, 8, count.index + 2)
  availability_zone = element(["us-east-1a", "us-east-1b"], count.index)

  tags = {
    Name = "devopsshack-private-${count.index}"
  }
}
```

* Creates **2 private subnets** for worker nodes.
* Nodes in private subnets **cannot be reached directly from the internet**.
* `cidrsubnet(..., count.index + 2)` ‚Üí avoids overlapping with public subnets.

---

## **5Ô∏è‚É£ Internet Gateway**

```hcl
resource "aws_internet_gateway" "devopsshack_igw" {
  vpc_id = aws_vpc.devopsshack_vpc.id

  tags = {
    Name = "devopsshack-igw"
  }
}
```

* Creates an **Internet Gateway** to allow internet access for public subnets (and NAT).

---

## **6Ô∏è‚É£ Public Route Table**

```hcl
resource "aws_route_table" "devopsshack_public_rt" {
  vpc_id = aws_vpc.devopsshack_vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.devopsshack_igw.id
  }

  tags = {
    Name = "devopsshack-public-rt"
  }
}

resource "aws_route_table_association" "public_assoc" {
  count          = 2
  subnet_id      = aws_subnet.devopsshack_public_subnet[count.index].id
  route_table_id = aws_route_table.devopsshack_public_rt.id
}
```

* Creates a **route table** for public subnets.
* `0.0.0.0/0` ‚Üí default route to the internet.
* Associates **each public subnet** with this route table.

---

## **7Ô∏è‚É£ NAT Gateways**

```hcl
resource "aws_eip" "nat_eip" {
  count = 2
  vpc   = true
}

resource "aws_nat_gateway" "devopsshack_nat" {
  count         = 2
  allocation_id = aws_eip.nat_eip[count.index].id
  subnet_id     = aws_subnet.devopsshack_public_subnet[count.index].id

  tags = {
    Name = "devopsshack-nat-${count.index}"
  }
}
```

* Creates **2 NAT Gateways** (1 per AZ) for private subnets to access the internet.
* `aws_eip` ‚Üí Elastic IPs assigned to NAT gateways (required for internet access).
* Private nodes can now pull updates or Docker images.

---

## **8Ô∏è‚É£ Private Route Tables**

```hcl
resource "aws_route_table" "devopsshack_private_rt" {
  count  = 2
  vpc_id = aws_vpc.devopsshack_vpc.id

  route {
    cidr_block     = "0.0.0.0/0"
    nat_gateway_id = aws_nat_gateway.devopsshack_nat[count.index].id
  }

  tags = {
    Name = "devopsshack-private-rt-${count.index}"
  }
}

resource "aws_route_table_association" "private_assoc" {
  count          = 2
  subnet_id      = aws_subnet.devopsshack_private_subnet[count.index].id
  route_table_id = aws_route_table.devopsshack_private_rt[count.index].id
}
```

* Creates **private route tables** for private subnets.
* Routes internet-bound traffic through **NAT Gateways**.
* Associates private subnets with their route tables.

---

## **9Ô∏è‚É£ Cluster Security Group**

```hcl
resource "aws_security_group" "devopsshack_cluster_sg" {
  vpc_id = aws_vpc.devopsshack_vpc.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "devopsshack-cluster-sg"
  }
}
```

* One security group for the **cluster and nodes**.
* `egress` allows **all outbound traffic**.
* No SSH ingress ‚Äî safer and sufficient for Kubernetes management.

---

## **üîü IAM Roles**

* **Cluster Role:** allows EKS control plane to manage AWS resources.
* **Node Group Role:** allows worker nodes to pull images, attach volumes, use networking.

Example:

```hcl
resource "aws_iam_role_policy_attachment" "devopsshack_node_ecr" {
  role       = aws_iam_role.devopsshack_node_group_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}
```

* Attaches AWS-managed IAM policies.
* `AmazonEC2ContainerRegistryReadOnly` ‚Üí allows nodes to pull Docker images from ECR.
* You **don‚Äôt memorize these ARNs**, just know the purpose.

---

## **1Ô∏è‚É£1Ô∏è‚É£ EKS Cluster**

```hcl
resource "aws_eks_cluster" "devopsshack" {
  name     = "devopsshack-cluster"
  role_arn = aws_iam_role.devopsshack_cluster_role.arn

  vpc_config {
    subnet_ids         = aws_subnet.devopsshack_private_subnet[*].id
    security_group_ids = [aws_security_group.devopsshack_cluster_sg.id]
  }

  depends_on = [aws_iam_role_policy_attachment.devopsshack_cluster_policy]
}
```

* Creates the **EKS control plane**.
* Uses **private subnets** for worker nodes.
* `depends_on` ensures IAM policies are attached first.

---

## **1Ô∏è‚É£2Ô∏è‚É£ EKS Addon (EBS CSI Driver)**

```hcl
resource "aws_eks_addon" "ebs_csi_driver" {
  cluster_name                = aws_eks_cluster.devopsshack.name
  addon_name                  = "aws-ebs-csi-driver"
  resolve_conflicts_on_create = "OVERWRITE"
  resolve_conflicts_on_update = "OVERWRITE"
}
```

* Adds **EBS CSI driver** to cluster.
* Allows pods to dynamically provision **EBS volumes**.

---

## **1Ô∏è‚É£3Ô∏è‚É£ Node Group**

```hcl
resource "aws_eks_node_group" "devopsshack" {
  cluster_name    = aws_eks_cluster.devopsshack.name
  node_group_name = "devopsshack-node-group"
  node_role_arn   = aws_iam_role.devopsshack_node_group_role.arn
  subnet_ids      = aws_subnet.devopsshack_private_subnet[*].id

  scaling_config {
    desired_size = 3
    max_size     = 5
    min_size     = 2
  }

  instance_types = ["t3.medium"]

  remote_access {
    ec2_ssh_key = null  # SSH disabled
  }

  depends_on = [aws_eks_cluster.devopsshack]
}
```

* Creates the **worker nodes** for your cluster.
* Nodes run in **private subnets** and use the **cluster security group**.
* `scaling_config` ‚Üí autoscaling between 2‚Äì5 nodes.
* `instance_types` ‚Üí EC2 type for nodes (`t3.medium`).

---

### ‚úÖ Summary

* **VPC** ‚Üí network
* **Public subnets** ‚Üí for NAT/IGW
* **Private subnets** ‚Üí for worker nodes
* **NAT Gateway** ‚Üí private nodes internet access
* **Cluster Security Group** ‚Üí used for both cluster and nodes
* **IAM Roles** ‚Üí cluster and nodes permissions
* **EKS Cluster** ‚Üí managed control plane
* **Node Group** ‚Üí EC2 instances running your pods
* **Addons** ‚Üí EBS CSI driver for persistent storage

---
